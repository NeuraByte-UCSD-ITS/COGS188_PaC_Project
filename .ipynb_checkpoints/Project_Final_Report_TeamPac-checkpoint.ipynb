{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec011056",
   "metadata": {},
   "source": [
    "# $$The\\;Pac$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce911dff",
   "metadata": {},
   "source": [
    "## Names\n",
    "\n",
    "- Edwin Ruiz\n",
    "- Bradley Grace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a185c",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This project aimed at developing an intelligent agent that navigates the Atari game Ms. PacMan efficiently using reinforcement learning algorithms within the OpenAI Gym environment. The primary dataset is the game states, which are Ms. PacMan’s position, the ghosts, and the dots. We implemented three types of algorithms: the neural network-based Deep-Q Network (DQN), an enhanced variant called Double Deep-Q Network (Double DQN), and the foundational Q-learning (QL) algorithm. Furthermore, we evaluated the DQN algorithm in three variations: pure exploration DQN, pure exploitation DQN, and DQN with randomly tuned hyperparameters. The agent's effectiveness was then assessed using four metrics: Average Reward, Convergence Rate, Stability, and Computation Time. Briefly, Average Reward corresponds to the game scores achieved by the agent; Convergence Rate measures how quickly the agent's performance stabilizes, which reflects its level completion rates; Stability indicates the consistency of the agent's performance over time, thereby representing its efficiency in preserving lives during gameplay; and Computation Time captures the computational efficiency of training each algorithm. Our results indicate that the DQN algorithm generally outperformed the Q-learning algorithm but did not show significant superiority over the Double DQN or other DQN variations. This suggests that further analysis and fine-tuning of all DQN models, and Double DQN, are necessary to determine the most sufficient algorithm for the Ms. PacMan environment. However, our efforts were limited by computational resources, thereby emphasizing the need for sufficient resources for high-level training and evaluation. Still, the significance of this research originates from its potential to advance the field of reinforcement learning by providing understanding into the effectiveness of various RL algorithms in intricate, dynamic environments; and by analyzing the performance differences between DQN, Double DQN, and Q-learning, this project contributes to understanding how these algorithms can be optimized for real-world applications, such as autonomous navigation and decision-making in unpredictable settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf6d58",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Ms.Pacman is a widely known, iconic arcade game. It is also popular in the field of computational research like artificial intelligence and reinforcement learning. This is because the dynamic constraints of the environment make it an excellent research tool for evaluating reinforcement learning algorithms such as Deep Q-Networks (DQNs) and Q-learning. Both of which are good in navigating environments like Ms.PacMan states because they can balance the exploration of new strategies by making use of known paths, according to the environmental feedback received. Consequently, this game is a great educational tool for students looking to explore and experiment with diffent AI methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70008dd3",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The primary problem addressed in this project is the development of a reinforcement learning agent capable of efficiently navigating and succeeding in the dynamic environment of the Atari game Ms. PacMan. This problem is significant because Ms.PacMan's game environment has several challenges. The first is the high dimensionality of the state space that is represented as pixel arrays, thus making it difficult for traditional algorithms to process it efficiently. The second challenge is the dynamic obstacles as moving ghosts that the agent must avoid while pursuing its goal of accumulating points and dodging or eating the ghost for more points. The third challenge is the sparse rewards that the agent receives intermittenly through, again, eating the dots, and eating the big dots to turn the ghost blue and be able to eat them for bigger rewards. This reward structure in the environment complicates the learning process. Hence, the goal is to enhance the agent's real-time decision-making abilities to achieve higher game scores, measured through the agent's performance metrics, and ensure that the solution is reproducible through the OpenAI Gym's simulation interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5ff79",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. How effectively can a DQN-based agent navigate the game of Ms. PacMan compared to Double DQN and Q-learning algorithms?\n",
    "2. What are the impacts of different exploration and exploitation strategies on the performance of the DQN-based agent?\n",
    "3. How do randomly tuned hyperparameters affect the performance of the DQN-based agent?\n",
    "4. What computational resources are required to train these reinforcement learning models to achieve optimal performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909beb0a",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "1. The DQN-based agent will outperform the Q-learning algorithm in terms of average reward and stability.\n",
    "2. The Double DQN will show improved performance over the DQN by addressing the overestimation bias in Q-values.\n",
    "3. Pure exploitation strategies will lead to faster convergence but lower stability compared to exploration strategies.\n",
    "4. Fine-tuning the hyperparameters of the DQN will result in improved performance compared to default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb6673",
   "metadata": {},
   "source": [
    "## Definitions and Variables\n",
    "- **State**: visual representation of Ms. PacMan’s current position, the ghosts, and the remaining dots.\n",
    "- **Action**: moves available to Ms. PacMan (e.g., up, down, left, right).\n",
    "- **Reward**: game score achieved based on Ms. PacMan’s actions.\n",
    "- **Convergence Rate**: number of timesteps required for the algorithm to stabilize to a steady policy.\n",
    "- **Stability**: variance in the rewards over time for consistency in performance\n",
    "- **Computation Time**: total time taken to train the algorithm until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082a26e",
   "metadata": {},
   "source": [
    "## Controls\n",
    "\n",
    "- **Algorithms Selection**: we selected a variety of reinforcement learning algorithms (DQN, Double DQN, and Q-learning) to compare their performance under identical conditions, which helped our understanding of which algorithm is best suited for the Ms. PacMan environment\n",
    "- **Preprocessing**: preprocessing of game state data to grayscale and downsampled pixel arrays remained consistent to ensure that each algorithm received input in the same format to reduce variability in the learning outcomes.\n",
    "- **Hyperparameter Tuning**: evaluating the DQN algorithm in three variations (pure exploration, pure exploitation, and random hyperparameters) allowed us to control the effects of different hyperparameter settings on the agent's performance.\n",
    "- **Evaluation Metrics**: standardized metrics (Average Reward, Convergence Rate, Stability, and Computation Time) were used to evaluate the performance of each algorithm to maintain a consistent basis for comparison and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e4845",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data used in this project is the live state generated by the OpenAI Gym environment for the Atari-Pac-Man game. The data source/environment can be found here [OpenAI Gym Ms. Pac-Man](https://gymnasium.farama.org/environments/atari/pacman/ \" Gym Ms. Pac-Man\"). Each state is captured as a pixel array that visually represents Ms. PacMan’s current position, the ghosts, and the remaining dots. The image frame is preprocessed using the following method from the `q_dqn_doubledqn.py` file:\n",
    " \n",
    " ```python\n",
    "def preprocess_image_frame(image_frame):\n",
    "    grayscale_image_frame = np.mean(image_frame, axis=2).astype(np.uint8)\n",
    "    downsampled_image_frame = grayscale_image_frame[::2, ::2]\n",
    "    return downsampled_image_frame\n",
    "```\n",
    "\n",
    "Key variables include the pixel data, the current game score, and the number of lives left. The state data is discretized using the following method from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "```python \n",
    "def discretize_observation_state(observation_state, bucket_thresholds):\n",
    "    flattened_state = observation_state.flatten()\n",
    "    bucket_indices = np.digitize(flattened_state, bucket_thresholds) - 1\n",
    "    max_index = len(bucket_thresholds) - 1\n",
    "    bucket_indices = np.clip(bucket_indices, 0, max_index)\n",
    "    return bucket_indices\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ced0b",
   "metadata": {},
   "source": [
    "## Proposed Solution\n",
    "\n",
    "We used Deep Q-Networks (DQNs) to train the Ms.PacMan agent. The agent model was trained iteratively by giving the agent rewards based on improvements of the game score and penalties for losing lives. The training process was implemented using Python and TensorFlow, and the rewards and penalties were done in the `q_dqn_doubledqn.py` file with the following code:\n",
    "\n",
    "```python\n",
    "    total_episode_reward += reward\n",
    "    if done or truncated:\n",
    "        break\n",
    "total_rewards.append(total_episode_reward)\n",
    "```\n",
    "In addition, we implemented a simple Q-learning agent to evaluate the improvements achieved by the DQNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29eb32",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "1. **Average Reward** was obtained per episode over a fixed number of episodes and was calculated as the mean of the total rewards over a number of episodes using the following formula and code from the the `evaluate_dqn.py` file:\n",
    " \n",
    "$$\\text{Average Reward} = \\frac{\\sum \\text{Total Rewards}}{\\text{Number of Episodes}}$$ \n",
    "\n",
    "```python\n",
    "avg_reward = np.mean(total_rewards)\n",
    "```\n",
    "2. **Convergence Rate** is the number of timesteps required for the algorithm to converge to a stable policy and it measures how quickly the agent's performance stabilizes, as calculated using the following method in the `q_dqn_doubledqn.py` file\n",
    "\n",
    "$$\\text{Convergence Rate} = \\min \\{ t \\mid \\forall \\epsilon \\in (0, \\text{threshold}), |\\text{mean\\_rewards}[t+i] - \\text{mean\\_rewards}[t]| < \\epsilon \\}$$\n",
    "\n",
    "```python\n",
    "def compute_convergence_rate(total_rewards, threshold=0.01):\n",
    "    mean_rewards = np.convolve(total_rewards, np.ones((100,))/100, mode='valid')\n",
    "    if len(mean_rewards) == 0:\n",
    "        return len(total_rewards)\n",
    "    convergence_episode = np.where(np.abs(np.diff(mean_rewards)) < threshold)[0]\n",
    "    return convergence_episode[0] if len(convergence_episode) > 0 else len(total_rewards)\n",
    "```\n",
    "\n",
    "3. **Stability** is the variance in the rewards over time, which indicate how consistently the algorithm performs once it has converged. This was measured as the standard deviation of the rewards, using the following method from the `evaluate_dqn.py` file:\n",
    "\n",
    "$$\\text{Stability} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (R_i - \\bar{R})^2}$$\n",
    "\n",
    "```python\n",
    "def compute_stability(total_rewards):\n",
    "    mean_rewards = np.convolve(total_rewards, np.ones((100,))/100, mode='valid')\n",
    "    return np.std(mean_rewards)\n",
    "```\n",
    "4. **Computation Time** is the total time taken to train the algorithm until convergence, which captures the efficiency of training each algorithm and is calculated as the difference between the start and end times of the training process using the following from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "$$\\text{Computation Time} = \\text{end\\_time} - \\text{start\\_time}$$\n",
    "\n",
    "```python\n",
    "start_time = time.time()\n",
    "....\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbeb164",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "To conduct this study, several reinforcement learning models were trained and analyzed on the Ms. PacMan environment. In addition to Python and TensorFlow, other relevant methods were used, including NumPy for numerical computations and OpenCV for image processing, Pandas for data structures and data analysis, and Matplotlib for creating visualizations. This environment requires specific dependencies and compatibility between them, to install all necessary packages enter the following command from the root directory:\n",
    "\n",
    "```python\n",
    "python -m venv finalproject\n",
    "source finalproject/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "The 6 models trained are:\n",
    "\n",
    "   1. **DQN Model-Free** algorithm, which is a basic implementation of the DQN algorithm with default parameters. It was chosen as the baseline model to evaluate the performance of the other variations.Here is its implementation and parameters from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "```python\n",
    "agent = DeepQNetworkAgent(state_shape=state_shape, action_space_size=environment.action_space.n)\n",
    "```\n",
    "   \n",
    "   2. **DQN Exploitation** algorithm, which implements pure exploitation having the agent focus soley on utilizing known strategies to maximize rewards. It was expected to converge quickly but showed less stability with a higher variance in rewards. Here is its implementation and parameters from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "```python\n",
    "agent.exploration_rate = 0 #Pure exploitation\n",
    "```\n",
    "\n",
    "   3. **DQN Exploration** algorithm, which is designed to explore the environment extensively with a high exploration rate. It aimed to discover new strategies and potentially achieve higher rewards but required more time to converge. Here is its implementation and parameters from the `q_dqn_doubledqn.py` file:\n",
    "   \n",
    "```python\n",
    "agent.exploration_rate = 1 #Pure exploration\n",
    "```\n",
    "\n",
    "   4. **DQN Hyperparameters** algorithm, which was a test of different, random hyperparameters on the DQN performance. The goal with this implementation was to account for any optimal settings that can arise by chance.\n",
    "Here is its implementation and parameters from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "```python\n",
    "agent = DeepQNetworkAgent(state_shape=state_shape, action_space_size=environment.action_space.n, replay_buffer_size=50000, batch_size=128, learning_rate=0.0005)\n",
    "```\n",
    "\n",
    "   5. **Double DQN** algorithm, which was implemented to address the overestimation bias in Q-values found in DQN models by using two separate networks for action selection and evaluation. Here is its implementation and parameters from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "```python\n",
    "agent = DoubleDeepQNetworkAgent(state_shape=state_shape, action_space_size=environment.action_space.n)\n",
    "```\n",
    "\n",
    "   6. **Q-learning** algorithm, which does not require representation of the environment and directly leans the value of the state-action pairs. It was include for comparison purposes to evaluate the relative performance of the better models for the multi-variable Ms.Pacman environment. Here is its implementation and parameters from the `q_dqn_doubledqn.py` file:\n",
    "\n",
    "```python\n",
    "agent = QLearningAgent(state_space_size=state_space_size, action_space_size=environment.action_space.n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c6da8",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The performance of different models was evaluated based on the four metrics: Average Rewards, Convergance Rate, Stability, and Computation Time, which is summarized in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bf5c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Reward</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Computation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DQN Model-Free</td>\n",
       "      <td>257.00</td>\n",
       "      <td>110.891839</td>\n",
       "      <td>10952.298332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DQN Exploitation</td>\n",
       "      <td>252.48</td>\n",
       "      <td>147.482370</td>\n",
       "      <td>10766.112784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DQN Exploration</td>\n",
       "      <td>256.12</td>\n",
       "      <td>128.206652</td>\n",
       "      <td>9205.606376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DQN Hyperparameters</td>\n",
       "      <td>248.14</td>\n",
       "      <td>97.050195</td>\n",
       "      <td>35788.864257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Double DQN</td>\n",
       "      <td>253.88</td>\n",
       "      <td>112.595495</td>\n",
       "      <td>10981.620007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q-learning</td>\n",
       "      <td>252.92</td>\n",
       "      <td>96.311337</td>\n",
       "      <td>9650.085551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Average Reward   Stability  Computation Time\n",
       "0       DQN Model-Free          257.00  110.891839      10952.298332\n",
       "1     DQN Exploitation          252.48  147.482370      10766.112784\n",
       "2      DQN Exploration          256.12  128.206652       9205.606376\n",
       "3  DQN Hyperparameters          248.14   97.050195      35788.864257\n",
       "4           Double DQN          253.88  112.595495      10981.620007\n",
       "5           Q-learning          252.92   96.311337       9650.085551"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'scripts/q_dqn_doubledqn_results/evaluation_results.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9011c59",
   "metadata": {},
   "source": [
    "The best-performing model was the DQN Model-Free, which achieved the highest average reward of 257.0, indicating it was the most successful at navigating the game. It also showed a relatively high stability of 110.89, meaning it maintained consistent performance over time, and a computation time of 10952.30 seconds, which was reasonable compared to other models. However, the performance of the DQN Model-Free was not significantly different from other DQN variations or the Double DQN. For example, the Double DQN had an average reward of 253.88 and a stability of 112.60, showing comparable performance. The lowest-performing algorithm in terms of average reward was the DQN with randomly tuned hyperparameters, with an average reward of 248.14 and the highest computation time of 35788.86 seconds. For these reasons, we selected the DQN Model-Free algorithm for fine-tuning since it balanced performance and efficiency by giving the highest average reward and maintaing acceptable stability and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526a8a8",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "\n",
    "During the implementaiton of the fine-tuning process, we experienced an execution error described as `zsh: killed python train_finetuned_dqn.py` in the terminal, which indicates that the process was terminated by the operating system. We determined probable causes to be either memory limits which caused the error due to the script using too much .... memory. Another possible reason is CPU limits which can cause the system to terminate the script if it is affecting overl system performance. However, the operating system was an Apple M1 arm64 Max Macbook with 64 GB RAM memory and 8 TB storage memory system, so the code needs to be further analyzed for possible bugs. This limitation prevented us from further fine-tuning the DQN Model-Free implementation, which demonstrate the need for sufficient computational resources for high-level training and evaluation, or Efficient Computing Techniques that leverage available hardware resources to process large datasets and training of large models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22358a56",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, our research demonstrated the effectiveness of various reinforcement learning models in navigating the Ms. PacMan environment. The DQN Model-Free algorithm showed fast learning capabilities, and resulted in the highest average reward among the models tested. The Double DQN algorithm provided high stability and addressed the overestimation bias issue in the traditional DQNs. The Q-learning model proved to be highly efficient in terms of training time. Furthemore, this project highlights the potential of reinforcement learning algorithms in handling dynamic environments. Eventhough not significant, we improved the DQN Model-Free algorithm after the first fine-tuning, which shows that adjustment of hyperparameters and networks can lead to gains. The limitations we experienced during the second fine-tuning implementation put emphasis on the importance of having access to sufficient computational power for training and producing more sophisticated models.\n",
    "\n",
    "Future work will focus on exploring more advanced hyperparameter optimization techniques, utilizing more computational resources for training, implementing and evaluating additional reinforcement learning algorithms, and expanding the scope to other complex and dynamic environments to generalize the findings. Specifically, the potential for real-world applications is significant. If optimal solutions are achieved and further enhancements are made, such models could be applied to autonomous navigation and decision-making in unpredictable settings, such as self-driving cars, robotics, and other AI-driven autonomous systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271f94a",
   "metadata": {},
   "source": [
    "## Ethics & Privacy\n",
    "\n",
    "**A. Data Collection**\n",
    "- [x] A.1 Limit PII exposure: Our project involves no personal data as it utilizes publicly available simulations, via OpenAI Gym. \n",
    "\n",
    "**B. Data Storage**\n",
    "- [x] B.1 Data security: We will securely store training data and models on GitHub, which will be made public after the project's completion.\n",
    "\n",
    "**C. Analysis**\n",
    "- [x] C.1 Dataset bias: We will monitor for possible biases from the agent for discovering possible game flaws.\n",
    "- [x] C.2 Honest representation: We will present all results and performance metrics truthfully and accurately.\n",
    "- [x] C.3 Auditability: We will assure reproducibility through complete documentation.\n",
    "\n",
    "**D. Modeling**\n",
    "- [x] D.1 Explainability: All strategic decisions by the agent will be thoroughly explained.\n",
    "- [x] D.2 Communicate limitations: We will clearly document any model limitations.\n",
    "\n",
    "**E. Deployment**\n",
    "- [x] E.1 Monitoring and evaluation: The model will be regularly monitored and updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170a6d7",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [DQN — Stable Baselines 2.10.3a0 documentation](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html)\n",
    "2. [The Deep Q-Learning Algorithm](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm)\n",
    "3. [Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "4. [Deep Q Learning Examples](https://github.com/rajibhossen/dqn-examples)\n",
    "5. [Deep-Q-Network](https://github.com/topics/deep-q-network)\n",
    "6. [17.3. Q-Learning](https://d2l.ai/chapter_reinforcement-learning/qlearning.html)\n",
    "7. [Introducing Q-Learning](https://huggingface.co/learn/deep-rl-course/en/unit2/q-learning)\n",
    "8. [Reinforcement Learning](https://ml-cheatsheet.readthedocs.io/en/latest/reinforcement_learning.html)\n",
    "9. [How to Train Ms-Pacman with Reinforcement Learning](https://medium.com/analytics-vidhya/how-to-train-ms-pacman-with-reinforcement-learning-dea714a2365e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
